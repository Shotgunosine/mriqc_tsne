{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up downloaded data and merge with other data sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-01-19T20:48:48.757898Z",
     "start_time": "2018-01-19T20:48:47.575058Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from json import load\n",
    "import urllib.request, json \n",
    "from pandas.io.json import json_normalize\n",
    "import seaborn as sns\n",
    "import pylab as plt\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "from pathlib import Path\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "from urllib.error import HTTPError\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dollarsigns(df):\n",
    "    replacements = {'_created.$date':'_created', '_id.$oid':'_id', '_updated.$date':'_updated'}\n",
    "    df = df.rename(columns=replacements)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load csvs with all MRIQC data\n",
    "Either downloaded through the api, from Chris Gorgolewski's Kaggle page: https://www.kaggle.com/chrisfilo/mriqc or from the OSF directory OHBM2019-TSNE-Analysis in the MRIQC project directory here: https://osf.io/haf97/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csvs\n",
    "df_t1w = pd.read_csv('all_t1s_2018_12_26.csv', index_col=0, low_memory=False)\n",
    "df_t2w = pd.read_csv('all_t2s_2018_12_26.csv', index_col=0, low_memory=False)\n",
    "df_bold = pd.read_csv('all_bolds_2018_12_26.csv', index_col=0, low_memory=False)\n",
    "\n",
    "df_t1w = clean_dollarsigns(df_t1w)\n",
    "df_t2w = clean_dollarsigns(df_t2w)\n",
    "df_bold = clean_dollarsigns(df_bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load extra data from local runs of mriqc\n",
    "json_dir = Path('jsons/')\n",
    "HCP_dir = json_dir / 'HCP'\n",
    "SALD_dir = json_dir / 'SALD'\n",
    "NNDSP_dir = json_dir / 'NNDSP'\n",
    "\n",
    "for ds, ds_dir in [('hcp', HCP_dir), ('sald', SALD_dir), ('nndsp', NNDSP_dir)]:\n",
    "    \n",
    "    dat = []\n",
    "    for js in ds_dir.glob('*_T1w.json'):\n",
    "        dat.append(json_normalize(json.loads(js.read_text())))\n",
    "    dat = pd.concat(dat, ignore_index=True, sort=False)\n",
    "    dat['dataset_lr'] = ds\n",
    "    df_t1w = pd.concat([df_t1w, dat], sort=False)\n",
    "\n",
    "for ds, ds_dir in [('nndsp', NNDSP_dir)]:\n",
    "    \n",
    "    dat = []\n",
    "    for js in ds_dir.glob('*_bold.json'):\n",
    "        dat.append(json_normalize(json.loads(js.read_text())))\n",
    "    dat = pd.concat(dat, ignore_index=True, sort=False)\n",
    "    dat['dataset_lr'] = ds\n",
    "    df_bold = pd.concat([df_bold, dat], sort=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count up the number of null fields and drop duplicates while keeping the rows with the fewest nans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1w['null_count'] = df_t1w.isnull().sum(1)\n",
    "df_t2w['null_count'] = df_t2w.isnull().sum(1)\n",
    "df_bold['null_count'] = df_bold.isnull().sum(1)\n",
    "\n",
    "# Sort so that rows with fewest nulls and most recent creation are towards the top\n",
    "df_t1w_unique = df_t1w.sort_values(['null_count','_created'], ascending=[True, False]).drop_duplicates(subset=['provenance.md5sum'])\n",
    "df_t2w_unique = df_t2w.sort_values(['null_count','_created'], ascending=[True, False]).drop_duplicates(subset=['provenance.md5sum'])\n",
    "df_bold_unique = df_bold.sort_values(['null_count','_created'], ascending=[True, False]).drop_duplicates(subset=['provenance.md5sum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1034572, 229) (52980, 229)\n",
      "(156543, 227) (62512, 227)\n"
     ]
    }
   ],
   "source": [
    "print(df_t1w.shape, df_t1w_unique.shape)\n",
    "print(df_bold.shape, df_bold_unique.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load up datalad metadata extracted on 12-26-2018, see the get_datalad_data notebook to prepare this data yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_df = pd.read_csv('datalad_metadata.csv', low_memory=False)\n",
    "dl_df['hash'] = dl_df.loc[dl_df.metadata__annex__key.notnull(),'metadata__annex__key'].str.split('--').str[-1].str.split('.').str[0]\n",
    "dl_df['hashing_algo'] = dl_df.loc[dl_df.metadata__annex__key.notnull(),'metadata__annex__key'].str.split('--').str[0].str.split('-').str[0]\n",
    "dl_df=dl_df.drop_duplicates(subset=['hashing_algo', 'hash'])\n",
    "\n",
    "df_t1w_dl_merge = df_t1w_unique.loc[~df_t1w_unique['provenance.settings.testing'], :].merge(dl_df.loc[dl_df.hashing_algo == 'MD5E', :], indicator=True, how='left', left_on='provenance.md5sum', right_on='hash')\n",
    "df_t2w_dl_merge = df_t2w_unique.loc[~df_t2w_unique['provenance.settings.testing'], :].merge(dl_df.loc[dl_df.hashing_algo == 'MD5E', :], indicator=True, how='left', left_on='provenance.md5sum', right_on='hash')\n",
    "df_bold_dl_merge = df_bold_unique.merge(dl_df.loc[dl_df.hashing_algo == 'MD5E', :], indicator=True, how='left', left_on='provenance.md5sum', right_on='hash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_etag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_merge</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left_only</th>\n",
       "      <td>47497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_only</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both</th>\n",
       "      <td>3728</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _etag\n",
       "_merge           \n",
       "left_only   47497\n",
       "right_only      0\n",
       "both         3728"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1w_dl_merge.groupby('_merge')[['_etag']].count()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_etag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_merge</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left_only</th>\n",
       "      <td>697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_only</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both</th>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _etag\n",
       "_merge           \n",
       "left_only     697\n",
       "right_only      0\n",
       "both           70"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t2w_dl_merge.groupby('_merge')[['_etag']].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>_etag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>_merge</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>left_only</th>\n",
       "      <td>51039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>right_only</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>both</th>\n",
       "      <td>10315</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            _etag\n",
       "_merge           \n",
       "left_only   51039\n",
       "right_only      0\n",
       "both        10315"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bold_dl_merge.groupby('_merge')[['_etag']].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load hashes calculated on local copies of semi-public datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Abide2' 'Abide' 'adhd200' 'CMI' 'CoRR' 'fcon1000' 'HCP' 'NNDSP'\n",
      " 'OpenNeuro' 'SALD']\n"
     ]
    }
   ],
   "source": [
    "dsst_datasets = pd.read_csv('ALL.txt', names=['dataset', 'hash','path'], delim_whitespace=True)\n",
    "print(dsst_datasets.dataset.unique())\n",
    "df_t1w_merge = df_t1w_dl_merge.merge(dsst_datasets, how='left', left_on='provenance.md5sum', right_on='hash', suffixes=('', '_HPC'), indicator='_dsst')\n",
    "df_t2w_merge = df_t2w_dl_merge.merge(dsst_datasets, how='left', left_on='provenance.md5sum', right_on='hash', suffixes=('', '_HPC'), indicator='_dsst')\n",
    "df_bold_merge = df_bold_dl_merge.merge(dsst_datasets, how='left', left_on='provenance.md5sum', right_on='hash', suffixes=('', '_HPC'), indicator='_dsst')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Define functions for cleaning up some of the bids meta fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_factor(df, new_names, column):\n",
    "    mlist = []\n",
    "    bad = []\n",
    "    for old_name in df[column]:\n",
    "        try:\n",
    "            mlist.append(new_names[old_name])\n",
    "        except KeyError:\n",
    "            if pd.notnull(old_name):\n",
    "                bad.append(old_name)\n",
    "            mlist.append(np.nan)\n",
    "    return bad, mlist\n",
    "\n",
    "model_dict = {'Signa HDe': 'Signa HDe',\n",
    "              'Signa_HDxt': 'Signa HDxt',\n",
    "              'Signa HDxt': 'Signa HDxt',\n",
    "              'SIGNA_HDx': 'Signa HDx',\n",
    "              'SIGNA_PET_MR': 'Signa PET-MR',\n",
    "              'SIGNA_EXCITE': 'Signa Excite',\n",
    "              'Signa Twin Speed Excite HD scanne': 'Signa Excite',\n",
    "              'SIGNA_Premier': 'Signa Premier',\n",
    "              'Signa Premier': 'Signa Premier',\n",
    "              'Signa': 'Signa',\n",
    "              'GENESIS_SIGNA': 'Signa Genesis',\n",
    "              'Symphony': 'Symphony',\n",
    "              'SymphonyTim': 'Symphony',\n",
    "              'Tim TRIO': 'Tim Trio',\n",
    "              'TrioTim': 'Tim Trio',\n",
    "              'Magnetom Trio' : 'Tim Trio',\n",
    "              'TIM TRIO': 'Tim Trio',\n",
    "              'MAGNETOM Trio': 'Tim Trio',\n",
    "              'TRIOTIM': 'Tim Trio',\n",
    "              'Trio': 'Tim Trio',\n",
    "              'Trio TIM': 'Tim Trio',\n",
    "              'Tim Trio': 'Tim Trio',\n",
    "              'TRIO': 'Tim Trio',\n",
    "              'MAGNETOM Trio A Tim': 'Tim Trio',\n",
    "              'TimTrio': 'Tim Trio',\n",
    "              'TriTim': 'Tim Trio',\n",
    "              'MAGNETOM and Jerry': 'MAGNETOM and Jerry',\n",
    "              'Trio Magnetom': 'Tim Trio',\n",
    "              'Prisma_fit': 'Prisma',\n",
    "              'Prisma': 'Prisma',\n",
    "              'Magnetom Skyra Fit': 'Skyra',\n",
    "              'MAGNETOM Skyra': 'Skyra',\n",
    "              'Skyra': 'Skyra',\n",
    "              'Intera': 'Intera',\n",
    "              'Allegra': 'Allegra',\n",
    "              'Verio': 'Verio',\n",
    "              'Avanto': 'Avanto',\n",
    "              'Sonata': 'Sonata',\n",
    "              'Espree': 'Espree',\n",
    "              'SonataVision': 'Sonata Vision',\n",
    "              'Spectra':'Spectra',\n",
    "              'Ingenia' : 'Ingenia',\n",
    "              'DISCOVERY MR750': 'Discovery MR750',\n",
    "              'DISCOVERY_MR750': 'Discovery MR750',\n",
    "              'DISCOVERY_MR750w': 'Discovery MR750',\n",
    "              'Discovery MR750': 'Discovery MR750',\n",
    "              'MR750': 'Discovery MR750',\n",
    "              'Achieva_dStream': 'Achieva dStream',\n",
    "              'Achieva dStream': 'Achieva dStream',\n",
    "              'Achieva Ds': 'Achieva dStream',\n",
    "              'Achieva': 'Achieva',\n",
    "              'Achieva TX': 'Achieva TX',\n",
    "              'Intera_Achieva': 'Achieva',\n",
    "              'Intera Achieva': 'Achieva',\n",
    "              'Philips Achieva': 'Achieva',\n",
    "              'GEMINI': 'Gemini',\n",
    "              'Ingenuity': 'Ingenuity',\n",
    "              'Gyroscan_Intera': 'Gyroscan Intera',\n",
    "              'Biograph_mMR': 'Biograph mMR',\n",
    "              'NUMARIS_4': 'Numaris 4',\n",
    "              'Investigational_Device_7T': 'Investigational 7T',\n",
    "              'N/A': np.nan,\n",
    "              '': np.nan,\n",
    "              'DicomCleaner': np.nan}\n",
    "\n",
    "mfg_dict = {'Siemens': 'Siemens',\n",
    "            'SIEMENS': 'Siemens',\n",
    "            'Simiens': 'Siemens',\n",
    "            'Siemans': 'Siemens',\n",
    "            'Simens': 'Siemens',\n",
    "            'GE': 'GE',\n",
    "            'G.E.': 'GE',\n",
    "            'GE MEDICAL SYSTEMS': 'GE',\n",
    "            'GE_MEDICAL_SYSTEMS': 'GE',\n",
    "            'General Electric': 'GE',\n",
    "            'General Electrics': 'GE',\n",
    "            'GE 3 Tesla MR750': 'GE',\n",
    "            'Philips':'Philips',\n",
    "            'Philips Ingenia 3.0T': 'Philips',\n",
    "            'Philips Achieva Intera 3 T Scanner': 'Philips', \n",
    "            'Philips Medical Systems': 'Philips',\n",
    "           }\n",
    "\n",
    "def clean_table(res_df):\n",
    "    # find all the rows with a mfg of 'GE 3 Tesla MR750' and make sure they've got a model value\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'GE 3 Tesla MR750', 'bids_meta.ManufacturersModelName'] = 'Discovery MR750'\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'GE 3 Tesla MR750', 'bids_meta.MagneticFieldStrength'] = 3.0\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'Philips Ingenia 3.0T', 'bids_meta.ManufacturersModelName'] = 'Ingenia'\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'Philips Ingenia 3.0T', 'bids_meta.MagneticFieldStrength'] = 3.0\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'Philips Achieva Intera 3 T Scanner', 'bids_meta.ManufacturersModelName'] = 'Achieva'\n",
    "    res_df.loc[res_df['bids_meta.Manufacturer'] == 'Philips Achieva Intera 3 T Scanner', 'bids_meta.MagneticFieldStrength'] = 3.0\n",
    "\n",
    "    bad, mlist = clean_factor(res_df, model_dict, 'bids_meta.ManufacturersModelName')\n",
    "    assert len(pd.unique(bad)) == 0\n",
    "    res_df['bids_meta.ManufacturersModelName'] = mlist\n",
    "\n",
    "    bad, mlist = clean_factor(res_df, mfg_dict , 'bids_meta.Manufacturer')\n",
    "    assert len(pd.unique(bad)) == 0\n",
    "    res_df['bids_meta.Manufacturer'] = mlist\n",
    "    \n",
    "    res_df['dataset_dl'] = res_df.path.str.split('/').str[6]\n",
    "    res_df['subdataset_dl'] = res_df.path.str.split('/').str[7]\n",
    "    res_df.loc[(res_df.dataset_dl == \"indi\"), 'subdataset_dl'] = res_df.path.str.split('/').str[7:9].str.join('__')\n",
    "    res_df['dataset'] = res_df.dataset.str.lower()\n",
    "    res_df.dataset = res_df.dataset.fillna(res_df.dataset_dl)\n",
    "    try:\n",
    "        res_df.dataset = res_df.dataset.fillna(res_df.dataset_lr)\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    res_df.loc[res_df.dataset == 'openneuro', 'subdataset'] = res_df.path_HPC.str.split('/').str[4]\n",
    "    res_df.subdataset = res_df.subdataset.fillna(res_df.subdataset_dl)\n",
    "    return res_df\n",
    "\n",
    "# Experimenting with pulling information out of additional fields\n",
    "\n",
    "# xs = ['bids_meta.PulseSequenceType', 'bids_meta.ScanningSequence',\n",
    "#       'bids_meta.SequenceVariant', 'bids_meta.ScanOptions',\n",
    "#       'bids_meta.PulseSequenceDetails', 'bids_meta.ScanOptions']\n",
    "# for x in xs:\n",
    "#     print(x, res_df[x].str.upper().unique(), res_df[x].notnull().sum())\n",
    "\n",
    "# #based on http://mriquestions.com/commercial-acronyms.html\n",
    "# sequence_dict = {'RF-Spoiled GRE': ((res_df['bids_meta.ProtocolName'].str.upper().str.contains('FLASH'))\n",
    "#                                | (res_df['bids_meta.ProtocolName'].str.upper().str.contains('SPGR'))\n",
    "#                                | (res_df['bids_meta.ProtocolName'].str.upper().str.contains('FFE'))),\n",
    "#               'Ultrafast GRE': ((res_df['bids_meta.ProtocolName'].str.upper().str.contains('RAGE'))\n",
    "#                                 | (res_df['bids_meta.ProtocolName'].str.upper().str.contains('MPR'))\n",
    "#                                 | (res_df['bids_meta.ProtocolName'].str.upper().str.contains('BRAVO'))\n",
    "#                                 | (res_df['bids_meta.ProtocolName'].str.upper().str.contains('TFE')))}\n",
    "# ssq_dict = {'RF-Spoiled GRE': ((res_df['bids_meta.ScanningSequence'] == 'IRSPGR')\n",
    "#                                   | (res_df['bids_meta.ScanningSequence'] == '3D SPGR')\n",
    "#                                   | (res_df['bids_meta.ScanningSequence'] == 'IR-FSPGR')),\n",
    "#              'Ultrafast GRE': ((res_df['bids_meta.ScanningSequence'] == 'MPRAGE')\n",
    "#                                   | (res_df['bids_meta.ScanningSequence'] == 'T1 turbo field echo')\n",
    "#                                   | (res_df['bids_meta.ScanningSequence'] == '3D TFE')) }\n",
    "\n",
    "# res_df['clean_sequence_name'] = np.nan\n",
    "# for name,ind in sequence_dict.items():\n",
    "#     res_df.loc[ind,'clean_sequence_name'] = name\n",
    "# for name,ind in ssq_dict.items():\n",
    "#     res_df.loc[ind,'clean_sequence_name'] = name\n",
    "\n",
    "\n",
    "# #cp_coil is single channel reference: https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.20925\n",
    "# coil_dict = {8: (res_df['bids_meta.ReceiveCoilName'].str.contains('8')\n",
    "#                 & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              12: ((res_df['bids_meta.ReceiveCoilName'].str.contains('12')\n",
    "#                   | res_df['bids_meta.ReceiveCoilName'].str.contains('HeadMatrix'))\n",
    "#                  & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              16: (res_df['bids_meta.ReceiveCoilName'].str.contains('16')\n",
    "#                  & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              20: (res_df['bids_meta.ReceiveCoilName'].str.contains('20')\n",
    "#                  & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              32: (res_df['bids_meta.ReceiveCoilName'].str.contains('32')\n",
    "#                  & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              64: (res_df['bids_meta.ReceiveCoilName'].str.contains('64')\n",
    "#                  & res_df['bids_meta.ReceiveCoilName'].notnull()),\n",
    "#              1: (res_df['bids_meta.ReceiveCoilName'].str.contains('CP')\n",
    "#                 & res_df['bids_meta.ReceiveCoilName'].notnull())}\n",
    "# for name,ind in coil_dict.items():\n",
    "#     res_df.loc[ind,'clean_receive_channels'] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1w_merge = clean_table(df_t1w_merge)\n",
    "df_t2w_merge = clean_table(df_t2w_merge)\n",
    "df_bold_merge = clean_table(df_bold_merge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1w_merge.to_csv('t1w_datalad_and_mriqc.csv', index=None)\n",
    "df_t2w_merge.to_csv('t2w_datalad_and_mriqc.csv', index=None)\n",
    "df_bold_merge.to_csv('bold_datalad_and_mriqc.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9177, 16998)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1w_merge.dataset.notnull().sum(), df_bold_merge.dataset.notnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "abide        1123\n",
       "abide2       1259\n",
       "cmi           982\n",
       "corr           41\n",
       "fcon1000       51\n",
       "hcp          1177\n",
       "nndsp         495\n",
       "openfmri      766\n",
       "openneuro    3076\n",
       "sald          207\n",
       "Name: provenance.md5sum, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_t1w_merge.groupby('dataset')['provenance.md5sum'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataset\n",
       "abide        1162\n",
       "abide2       1152\n",
       "cmi          2014\n",
       "fcon1000      497\n",
       "nndsp        1168\n",
       "openfmri     1330\n",
       "openneuro    9675\n",
       "Name: provenance.md5sum, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_bold_merge.groupby('dataset')['provenance.md5sum'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
